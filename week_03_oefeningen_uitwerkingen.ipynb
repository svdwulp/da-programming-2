{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis - Programming\n",
    "## Week 3\n",
    "## Oefeningen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening 1 <img src=\"images/stars_1.png\" alt=\"1 star\" style=\"display: inline; margin-top: -5px\" />\n",
    "\n",
    "Er zijn 36 mogelijke combinaties voor het aantal ogen dat je gooit met twee dobbelstenen ((1, 1), (1, 2), … , (6, 6)). Vaak is men geïnteresseerd in de som van het aantal ogen. Maak een dictionary met de som van het aantal ogen als key, en een list van tuples van alle bijbehorende combinaties als value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: [(1, 1)],\n",
       " 3: [(1, 2), (2, 1)],\n",
       " 4: [(1, 3), (2, 2), (3, 1)],\n",
       " 5: [(1, 4), (2, 3), (3, 2), (4, 1)],\n",
       " 6: [(1, 5), (2, 4), (3, 3), (4, 2), (5, 1)],\n",
       " 7: [(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)],\n",
       " 8: [(2, 6), (3, 5), (4, 4), (5, 3), (6, 2)],\n",
       " 9: [(3, 6), (4, 5), (5, 4), (6, 3)],\n",
       " 10: [(4, 6), (5, 5), (6, 4)],\n",
       " 11: [(5, 6), (6, 5)],\n",
       " 12: [(6, 6)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# opgave 1\n",
    "dice_sum = {}\n",
    "for d1 in range(1, 7):\n",
    "    for d2 in range(1, 7):\n",
    "        if (d1 + d2) in dice_sum:\n",
    "            dice_sum[d1 + d2].append((d1, d2))\n",
    "        else:\n",
    "            dice_sum[d1 + d2] = [(d1, d2)]\n",
    "dice_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Oefening 2\n",
    "\n",
    "a) Kun je het onderstaande dictionary afdrukken naar het scherm, waarbij gesorteerd wordt op keys? <img src=\"images/stars_1.png\" alt=\"1 star\" style=\"display: inline; margin-top: -5px\" />\n",
    "\n",
    "```python\n",
    "d = {\n",
    "    \"Nederland\": 7,\n",
    "    \"Frankrijk\": 4,\n",
    "    \"Belgie\": 8,\n",
    "    \"Duitsland\": 6,\n",
    "    \"Luxemburg\": 9,\n",
    "}\n",
    "```\n",
    "\n",
    "b) Kun je het dictionary uit a) ook afdrukken naar het scherm gesorteerd op value, aflopend? <img src=\"images/stars_2.png\" alt=\"2 star\" style=\"display: inline; margin-top: -5px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belgie 8\n",
      "Duitsland 6\n",
      "Frankrijk 4\n",
      "Luxemburg 9\n",
      "Nederland 7\n"
     ]
    }
   ],
   "source": [
    "# opgave 2a\n",
    "d = {\n",
    "    \"Nederland\": 7,\n",
    "    \"Frankrijk\": 4,\n",
    "    \"Belgie\": 8,\n",
    "    \"Duitsland\": 6,\n",
    "    \"Luxemburg\": 9,\n",
    "}\n",
    "for key, value in sorted(d.items()):\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luxemburg 9\n",
      "Belgie 8\n",
      "Nederland 7\n",
      "Duitsland 6\n",
      "Frankrijk 4\n"
     ]
    }
   ],
   "source": [
    "# opgave 2b\n",
    "d = {\n",
    "    \"Nederland\": 7,\n",
    "    \"Frankrijk\": 4,\n",
    "    \"Belgie\": 8,\n",
    "    \"Duitsland\": 6,\n",
    "    \"Luxemburg\": 9,\n",
    "}\n",
    "list_by_val = []\n",
    "for key, value in d.items():\n",
    "    list_by_val.append((value, key))\n",
    "for value, key in sorted(list_by_val, reverse=True):\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening 3\n",
    "\n",
    "a) Gebruik een dictionary om een frequentietabel te maken van de onderstaande  tekst (uit \"Alice's Adventures in Wonderland\" van Lewis Carroll). Probeer alleen de letters te tellen (niet de leestekens) en maak daarbij geen onderscheid tussen hoofd- en kleine letters. <img src=\"images/stars_1.png\" alt=\"1 star\" style=\"display: inline; margin-top: -5px\" />  \n",
    "Ter controle: er zitten 105 letters 'a' in de tekst, dus als je frequentietabel `freq` heet, zal `freq['a'] == 105` gelden.\n",
    "\n",
    "b) Kun je ook de woorden tellen in de tekst? Dus maak een frequentietabel waarbij de woorden uit de tekst de key zijn en hun frequentie de bijbehorende value. Probeer wederom leestekens en hoofd-/kleine letters te negeren. <img src=\"images/stars_2.png\" alt=\"2 star\" style=\"display: inline; margin-top: -5px\" />  \n",
    "Ter controle: het woord 'alice' komt 6 keer voor in de tekst.\n",
    "\n",
    "c) Zoek zelf uit wat het datatype (de class) `Counter` uit de module `collections` doet. Kun je de bovenstaande vragen (makkelijker) a) en b) implementeren met behulp van deze ingebouwde functionaliteit? <img src=\"images/stars_3.png\" alt=\"3 star\" style=\"display: inline; margin-top: -5px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_alice = \"\"\"ALICE was beginning to get very tired of sitting by her\n",
    "sister on the bank, and of having nothing to do: once or twice she had\n",
    "peeped into the book her sister was reading, but it had no pictures or\n",
    "conversations in it, and what is the use of a book, thought Alice,\n",
    "without pictures or conversations?\n",
    "\n",
    "So she was considering in her own mind (as well as she could, for the\n",
    "hot day made her feel very sleepy and stupid) whether the pleasure of\n",
    "making a daisy chain would be worth the trouble of getting up and\n",
    "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
    "close by her.\n",
    "\n",
    "There was nothing so very remarkable in that; nor did Alice think it\n",
    "so very much out of the way to hear the Rabbit say to itself, Oh\n",
    "dear! Oh dear! I shall be too late! (when she thought it over\n",
    "afterwards, it occurred to her that she ought to have wondered at this,\n",
    "but at the time it all seemed quite natural); but when the Rabbit\n",
    "actually took a watch out of its waistcoat pocket, and looked at it,\n",
    "and then hurried on, Alice started to her feet, for it flashed across\n",
    "her mind that she had never before seen a rabbit with either a\n",
    "waistcoat pocket, or a watch to take out of it, and burning with\n",
    "curiosity, she ran across the field after it, and was just in time to\n",
    "see it pop down a large rabbit hole under the hedge.\n",
    "\n",
    "In another moment down went Alice after it, never once considering how\n",
    "in the world she was to get out again.\n",
    "\n",
    "The rabbit hole went straight on like a tunnel for some way, and then\n",
    "dipped suddenly down, so suddenly that Alice had not a moment to think\n",
    "about stopping herself before she found herself falling down what seemed\n",
    "to be a very deep well.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'m': 15, 'r': 77, 's': 72, 'f': 25, 'g': 25, 'b': 30, 'i': 93, 'n': 86, 't': 143, 'e': 163, 'y': 19, 'k': 15, 'a': 105, 'l': 43, 'c': 32, 'v': 12, 'h': 88, 'd': 60, 'j': 1, 'p': 19, 'q': 1, 'u': 36, 'o': 104, 'w': 39}\n"
     ]
    }
   ],
   "source": [
    "# opgave 3a\n",
    "freq = {}\n",
    "for c in text_alice:\n",
    "    if c.isalpha():\n",
    "        if c.lower() in freq:\n",
    "            freq[c.lower()] += 1\n",
    "        else:\n",
    "            freq[c.lower()] = 1\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'remarkable': 1, 'close': 1, 'hole': 2, 'chain': 1, 'nor': 1, 'across': 2, 'did': 1, 'flashed': 1, 'conversations': 2, 'there': 1, 'falling': 1, 'was': 6, 'afterwards': 1, 'but': 3, 'have': 1, 'sitting': 1, 'up': 1, 'peeped': 1, 'shall': 1, 'much': 1, 'made': 1, 'mind': 2, 'is': 1, 'way': 2, 'actually': 1, 'dipped': 1, 'get': 2, 'whether': 1, 'do': 1, 'with': 3, 'then': 2, 'the': 15, 'burning': 1, 'picking': 1, 'down': 4, 'went': 2, 'waistcoat': 2, 'looked': 1, 'oh': 2, 'again': 1, 'out': 4, 'daisy': 1, 'some': 1, 'tunnel': 1, 'own': 1, 'and': 9, 'it': 12, 'getting': 1, 'eyes': 1, 'straight': 1, 'had': 4, 'not': 1, 'too': 1, 'pink': 1, 'considering': 2, 'this': 1, 'took': 1, 'stopping': 1, 'wondered': 1, 'bank': 1, 'itself': 1, 'occurred': 1, 'she': 9, 'as': 2, 'suddenly': 3, 'that': 4, 'twice': 1, 'trouble': 1, 'what': 2, 'of': 8, 'daisies': 1, 'curiosity': 1, 'under': 1, 'without': 1, 'day': 1, 'all': 1, 'deep': 1, 'hurried': 1, 'worth': 1, 'field': 1, 'quite': 1, 'how': 1, 'white': 1, 'for': 3, 'beginning': 1, 'large': 1, 'reading': 1, 'on': 3, 'herself': 2, 'before': 2, 'thought': 2, 'ought': 1, 'sister': 2, 'either': 1, 'a': 11, 'could': 1, 'would': 1, 'think': 2, 'time': 2, 'after': 2, 'see': 1, 'alice': 6, 'hot': 1, 'rabbit': 6, 'very': 5, 'be': 3, 'another': 1, 'dear': 2, 'hedge': 1, 'started': 1, 'so': 4, 'sleepy': 1, 'late': 1, 'when': 3, 'pleasure': 1, 'at': 3, 'to': 12, 'just': 1, 'like': 1, 'take': 1, 'feel': 1, 'i': 1, 'seen': 1, 'stupid': 1, 'ran': 2, 'pocket': 2, 'over': 1, 'or': 4, 'having': 1, 'into': 1, 'use': 1, 'found': 1, 'natural': 1, 'hear': 1, 'in': 6, 'moment': 2, 'feet': 1, 'once': 2, 'about': 1, 'world': 1, 'by': 2, 'nothing': 2, 'watch': 2, 'never': 2, 'its': 1, 'her': 8, 'tired': 1, 'seemed': 2, 'book': 2, 'pop': 1, 'making': 1, 'no': 1, 'say': 1, 'pictures': 2, 'well': 2}\n"
     ]
    }
   ],
   "source": [
    "# opgave 3b (zonder str.split())\n",
    "word_freq = {}\n",
    "word = \"\"\n",
    "for char in text_alice:\n",
    "    if char.isalpha():\n",
    "        word += char.lower()\n",
    "    elif len(word) > 0:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "        word = \"\"\n",
    "if len(word) > 0:\n",
    "    if word in word_freq:\n",
    "        word_freq[word] += 1\n",
    "    else:\n",
    "        word_freq[word] = 1\n",
    "\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'remarkable': 1, 'close': 1, 'hole': 2, 'chain': 1, 'nor': 1, 'across': 2, 'did': 1, 'flashed': 1, 'conversations': 2, 'there': 1, 'falling': 1, 'was': 6, 'afterwards': 1, 'but': 3, 'have': 1, 'sitting': 1, 'up': 1, 'peeped': 1, 'shall': 1, 'much': 1, 'made': 1, 'mind': 2, 'is': 1, 'way': 2, 'actually': 1, 'dipped': 1, 'get': 2, 'whether': 1, 'do': 1, 'with': 3, 'then': 2, 'the': 15, 'burning': 1, 'picking': 1, 'down': 4, 'went': 2, 'waistcoat': 2, 'looked': 1, 'oh': 2, 'again': 1, 'out': 4, 'daisy': 1, 'some': 1, 'tunnel': 1, 'own': 1, 'and': 9, 'it': 12, 'getting': 1, 'eyes': 1, 'straight': 1, 'had': 4, 'not': 1, 'too': 1, 'pink': 1, 'considering': 2, 'this': 1, 'took': 1, 'stopping': 1, 'wondered': 1, 'bank': 1, 'itself': 1, 'occurred': 1, 'she': 9, 'as': 2, 'suddenly': 3, 'that': 4, 'twice': 1, 'trouble': 1, 'what': 2, 'of': 8, 'daisies': 1, 'curiosity': 1, 'under': 1, 'without': 1, 'day': 1, 'all': 1, 'deep': 1, 'hurried': 1, 'worth': 1, 'field': 1, 'quite': 1, 'how': 1, 'white': 1, 'for': 3, 'beginning': 1, 'large': 1, 'reading': 1, 'on': 3, 'herself': 2, 'before': 2, 'thought': 2, 'ought': 1, 'sister': 2, 'either': 1, 'a': 11, 'could': 1, 'would': 1, 'think': 2, 'time': 2, 'after': 2, 'see': 1, 'alice': 6, 'hot': 1, 'rabbit': 6, 'very': 5, 'be': 3, 'another': 1, 'dear': 2, 'hedge': 1, 'started': 1, 'so': 4, 'sleepy': 1, 'late': 1, 'when': 3, 'pleasure': 1, 'at': 3, 'to': 12, 'just': 1, 'like': 1, 'take': 1, 'feel': 1, 'i': 1, 'seen': 1, 'stupid': 1, 'ran': 2, 'pocket': 2, 'over': 1, 'or': 4, 'having': 1, 'into': 1, 'use': 1, 'found': 1, 'natural': 1, 'hear': 1, 'in': 6, 'moment': 2, 'feet': 1, 'once': 2, 'about': 1, 'world': 1, 'by': 2, 'nothing': 2, 'watch': 2, 'never': 2, 'its': 1, 'her': 8, 'tired': 1, 'seemed': 2, 'book': 2, 'pop': 1, 'making': 1, 'no': 1, 'say': 1, 'pictures': 2, 'well': 2}\n"
     ]
    }
   ],
   "source": [
    "# opgave 3b (met str.split())\n",
    "filtered_chars = []\n",
    "prev_is_space = False\n",
    "for letter in text_alice:\n",
    "    if letter.isalpha():\n",
    "        filtered_chars.append(letter.lower())\n",
    "        prev_is_space = False\n",
    "    elif not prev_is_space:\n",
    "        filtered_chars.append(\" \")\n",
    "        prev_is_space = True\n",
    "filtered_text = \"\".join(filtered_chars).strip()\n",
    "\n",
    "words = filtered_text.split(\" \")\n",
    "word_freq = {}\n",
    "for word in words:\n",
    "    if word in word_freq:\n",
    "        word_freq[word] += 1\n",
    "    else:\n",
    "        word_freq[word] = 1\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 210\n",
      "b 60\n",
      "c 64\n",
      "d 120\n",
      "e 326\n",
      "f 50\n",
      "g 50\n",
      "h 176\n",
      "i 186\n",
      "j 2\n",
      "k 30\n",
      "l 86\n",
      "m 30\n",
      "n 172\n",
      "o 208\n",
      "p 38\n",
      "q 2\n",
      "r 154\n",
      "s 144\n",
      "t 286\n",
      "u 72\n",
      "v 24\n",
      "w 78\n",
      "x 0\n",
      "y 38\n",
      "z 0\n"
     ]
    }
   ],
   "source": [
    "# opgave 3c (Counter)\n",
    "# letter frequency example\n",
    "from collections import Counter\n",
    "from string import ascii_lowercase\n",
    "\n",
    "filter_chars = []\n",
    "prev_is_space = False\n",
    "for letter in text_alice:\n",
    "    if letter.isalpha():\n",
    "        filtered_chars.append(letter.lower())\n",
    "        prev_is_space = False\n",
    "    elif not prev_is_space:\n",
    "        filtered_chars.append(\" \")\n",
    "        prev_is_space = True\n",
    "filtered_text = \"\".join(filtered_chars).strip()\n",
    "\n",
    "letter_freq = Counter(filtered_text)\n",
    "for letter in ascii_lowercase:\n",
    "    print(letter, letter_freq[letter])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a about about about about about about across across across across across across across across across across across across actually actually actually actually actually actually after after after after after after after after after after after after afterwards afterwards afterwards afterwards afterwards afterwards again again again again again again alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice alice all all all all all all and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and another another another another another another as as as as as as as as as as as as at at at at at at at at at at at at at at at at at at bank bank bank bank bank bank be be be be be be be be be be be be be be be be be be before before before before before before before before before before before before beginning beginning beginning beginning beginning beginning book book book book book book book book book book book book burning burning burning burning burning burning but but but but but but but but but but but but but but but but but but by by by by by by by by by by by by chain chain chain chain chain chain close close close close close close considering considering considering considering considering considering considering considering considering considering considering considering conversations conversations conversations conversations conversations conversations conversations conversations conversations conversations conversations conversations could could could could could could curiosity curiosity curiosity curiosity curiosity curiosity daisies daisies daisies daisies daisies daisies daisy daisy daisy daisy daisy daisy day day day day day day dear dear dear dear dear dear dear dear dear dear dear dear deep deep deep deep deep deep did did did did did did dipped dipped dipped dipped dipped dipped do do do do do do down down down down down down down down down down down down down down down down down down down down down down down down either either either either either either eyes eyes eyes eyes eyes eyes falling falling falling falling falling falling feel feel feel feel feel feel feet feet feet feet feet feet field field field field field field flashed flashed flashed flashed flashed flashed for for for for for for for for for for for for for for for for for for found found found found found found get get get get get get get get get get get get getting getting getting getting getting getting had had had had had had had had had had had had had had had had had had had had had had had had have have have have have have having having having having having having hear hear hear hear hear hear hedge hedge hedge hedge hedge hedge her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her herself herself herself herself herself herself herself herself herself herself herself herself hole hole hole hole hole hole hole hole hole hole hole hole hot hot hot hot hot hot how how how how how how hurried hurried hurried hurried hurried hurried i i i i i i in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in into into into into into into is is is is is is it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it its its its its its its itself itself itself itself itself itself just just just just just just large large large large large large late late late late late late like like like like like like looked looked looked looked looked looked made made made made made made making making making making making making mind mind mind mind mind mind mind mind mind mind mind mind moment moment moment moment moment moment moment moment moment moment moment moment much much much much much much natural natural natural natural natural natural never never never never never never never never never never never never no no no no no no nor nor nor nor nor nor not not not not not not nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing nothing occurred occurred occurred occurred occurred occurred of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of oh oh oh oh oh oh oh oh oh oh oh oh on on on on on on on on on on on on on on on on on on once once once once once once once once once once once once or or or or or or or or or or or or or or or or or or or or or or or or ought ought ought ought ought ought out out out out out out out out out out out out out out out out out out out out out out out out over over over over over over own own own own own own peeped peeped peeped peeped peeped peeped picking picking picking picking picking picking pictures pictures pictures pictures pictures pictures pictures pictures pictures pictures pictures pictures pink pink pink pink pink pink pleasure pleasure pleasure pleasure pleasure pleasure pocket pocket pocket pocket pocket pocket pocket pocket pocket pocket pocket pocket pop pop pop pop pop pop quite quite quite quite quite quite rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit rabbit ran ran ran ran ran ran ran ran ran ran ran ran reading reading reading reading reading reading remarkable remarkable remarkable remarkable remarkable remarkable say say say say say say see see see see see see seemed seemed seemed seemed seemed seemed seemed seemed seemed seemed seemed seemed seen seen seen seen seen seen shall shall shall shall shall shall she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she she sister sister sister sister sister sister sister sister sister sister sister sister sitting sitting sitting sitting sitting sitting sleepy sleepy sleepy sleepy sleepy sleepy so so so so so so so so so so so so so so so so so so so so so so so so some some some some some some started started started started started started stopping stopping stopping stopping stopping stopping straight straight straight straight straight straight stupid stupid stupid stupid stupid stupid suddenly suddenly suddenly suddenly suddenly suddenly suddenly suddenly suddenly suddenly suddenly suddenly suddenly suddenly suddenly suddenly suddenly suddenly take take take take take take that that that that that that that that that that that that that that that that that that that that that that that that the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the then then then then then then then then then then then then there there there there there there think think think think think think think think think think think think this this this this this this thought thought thought thought thought thought thought thought thought thought thought thought time time time time time time time time time time time time tired tired tired tired tired tired to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to too too too too too too took took took took took took trouble trouble trouble trouble trouble trouble tunnel tunnel tunnel tunnel tunnel tunnel twice twice twice twice twice twice under under under under under under up up up up up up use use use use use use very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very waistcoat waistcoat waistcoat waistcoat waistcoat waistcoat waistcoat waistcoat waistcoat waistcoat waistcoat waistcoat was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was watch watch watch watch watch watch watch watch watch watch watch watch way way way way way way way way way way way way well well well well well well well well well well well well went went went went went went went went went went went went what what what what what what what what what what what what when when when when when when when when when when when when when when when when when when whether whether whether whether whether whether white white white white white white with with with with with with with with with with with with with with with with with with without without without without without without wondered wondered wondered wondered wondered wondered world world world world world world worth worth worth worth worth worth would would would would would would \n"
     ]
    }
   ],
   "source": [
    "# opgave 3c (Counter)\n",
    "# Alice in Wonderland, reading for neat freaks\n",
    "from collections import Counter\n",
    "\n",
    "filter_chars = []\n",
    "prev_is_space = False\n",
    "for letter in text_alice:\n",
    "    if letter.isalpha():\n",
    "        filtered_chars.append(letter.lower())\n",
    "        prev_is_space = False\n",
    "    elif not prev_is_space:\n",
    "        filtered_chars.append(\" \")\n",
    "        prev_is_space = True\n",
    "filtered_text = \"\".join(filtered_chars).strip()\n",
    "\n",
    "word_freq = Counter(filtered_text.split())\n",
    "for word in sorted(word_freq.elements()):\n",
    "    print(word, end=\" \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
